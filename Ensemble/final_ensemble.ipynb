{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do average voting using multiple LightGBM classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_path = 'models/voting/'\n",
    "output_path = 'output/final_prediction.csv'\n",
    "\n",
    "os.makedirs(model_dir_path, exist_ok=True)\n",
    "os.makedirs('output/', exist_ok=True)\n",
    "\n",
    "train_transaction_data_path = 'data/train_transaction.csv'\n",
    "train_identity_data_path = 'data/train_identity.csv'\n",
    "test_transaction_data_path = 'data/test_transaction.csv'\n",
    "test_identity_data_path = 'data/test_identity.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list down useless features (known from feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_features = [\n",
    "    'TransactionID',  # not really a feature\n",
    "    'dist2',  # transaction features\n",
    "    'C3',  # C features\n",
    "    'D6', 'D7', 'D8', 'D9', 'D12', 'D13', 'D14',  # D features\n",
    "    'M1',  # M features\n",
    "    'id_07', 'id_08', 'id_18', 'id_21', 'id_22', 'id_23',  # id features\n",
    "    'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_35',  # id features\n",
    "    'V6', 'V8', 'V9', 'V10', 'V11', 'V14', 'V15', 'V16',  # V features\n",
    "    'V18', 'V21', 'V22', 'V27', 'V28', 'V31', 'V32',  # V features\n",
    "    'V41', 'V42', 'V46', 'V50', 'V51', 'V59', 'V65',  # V features\n",
    "    'V68', 'V71', 'V72', 'V79', 'V80', 'V84', 'V85',  # V features\n",
    "    'V88', 'V89', 'V92', 'V93', 'V95', 'V98', 'V101',  # V features\n",
    "    'V104', 'V106', 'V107', 'V108', 'V109', 'V110',  # V features\n",
    "    'V111', 'V112', 'V113', 'V114', 'V116', 'V117',  # V features\n",
    "    'V118', 'V119', 'V120', 'V121', 'V122', 'V123',  # V features \n",
    "    'V125', 'V138', 'V141', 'V142', 'V144', 'V146',  # V features \n",
    "    'V147', 'V148', 'V151', 'V153', 'V154', 'V155',  # V features \n",
    "    'V157', 'V158', 'V159', 'V161', 'V163', 'V164',  # V features \n",
    "    'V166', 'V172', 'V173', 'V174', 'V175', 'V176',  # V features \n",
    "    'V177', 'V178', 'V179', 'V180', 'V181', 'V182',  # V features  \n",
    "    'V183', 'V184', 'V185', 'V186', 'V190', 'V191',  # V features  \n",
    "    'V192', 'V193', 'V194', 'V195', 'V196', 'V197',  # V features  \n",
    "    'V198', 'V199', 'V214', 'V216', 'V220', 'V225',  # V features \n",
    "    'V226', 'V227', 'V230', 'V233', 'V235', 'V236',  # V features  \n",
    "    'V237', 'V238', 'V239', 'V240', 'V241', 'V242',  # V features \n",
    "    'V244', 'V246', 'V247', 'V248', 'V249', 'V250',  # V features \n",
    "    'V252', 'V254', 'V255', 'V269', 'V276', 'V297',  # V features \n",
    "    'V300', 'V302', 'V304', 'V305', 'V325', 'V327',  # V features  \n",
    "    'V328', 'V329', 'V330', 'V334', 'V335', 'V336',  # V features \n",
    "    'V337', 'V338', 'V339',  # V features \n",
    "]\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define utility function to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce dataframe size\n",
    "\n",
    "    params:\n",
    "    - df: dataframe to reduce the size of\n",
    "\n",
    "    return:\n",
    "    - dataframe of reduced size\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'float128']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float64).min and c_max < np.finfo(np.float64).max:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                elif c_min > np.finfo(np.float128).min and c_max < np.finfo(np.float128).max:\n",
    "                    df[col] = df[col].astype(np.float128)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    if verbose: \n",
    "        print(\n",
    "            'Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(\n",
    "            end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "        ))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data() -> pd.DataFrame:\n",
    "    transaction_dataframe = pd.read_csv(train_transaction_data_path)\n",
    "    transaction_dataframe = reduce_mem_usage(transaction_dataframe)\n",
    "\n",
    "    identity_dataframe = pd.read_csv(train_identity_data_path)\n",
    "    identity_dataframe = reduce_mem_usage(identity_dataframe)\n",
    "\n",
    "    dataframe = transaction_dataframe.merge(identity_dataframe, how='outer')\n",
    "\n",
    "    print(f'number of rows in training data: {len(dataframe)}')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    transaction_dataframe = pd.read_csv(test_transaction_data_path)\n",
    "    transaction_dataframe = reduce_mem_usage(transaction_dataframe)\n",
    "\n",
    "    identity_dataframe = pd.read_csv(test_identity_data_path)\n",
    "    identity_dataframe = reduce_mem_usage(identity_dataframe)\n",
    "    identity_dataframe = identity_dataframe.rename(\n",
    "        columns={\n",
    "            column: column.replace('-', '_')\n",
    "            for column in identity_dataframe.columns\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dataframe = transaction_dataframe.merge(identity_dataframe, how='outer')\n",
    "    transaction_id_data = dataframe['TransactionID']  # need it for output\n",
    "\n",
    "    print(f'number of rows in test data: {len(dataframe)}')\n",
    "    return dataframe, transaction_id_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to disregard browser versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_browser_version(df: pd.DataFrame, verbose: bool=True):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - df (pd.DataFrame): has id_31 as one of its columns\n",
    "    - verbose (bool): prints information if True\n",
    "\n",
    "    return: dataframe, after browser versions have been ignored\n",
    "    \"\"\"\n",
    "    browser_list = [\n",
    "        'aol',\n",
    "        'chrome',\n",
    "        'chromium',\n",
    "        'comodo',\n",
    "        'cyberfox',\n",
    "        'edge',\n",
    "        'firefox',\n",
    "        'icedragon',\n",
    "        'ie',\n",
    "        'iron',\n",
    "        'maxthon',\n",
    "        'opera',\n",
    "        'palemoon',\n",
    "        'puffin',\n",
    "        'safari',\n",
    "        'samsung',\n",
    "        'seamonkey',\n",
    "        'silk',\n",
    "        'waterfox',\n",
    "    ]\n",
    "\n",
    "    for index, browser in df.id_31.iteritems():\n",
    "        if not isinstance(browser, str):\n",
    "            continue  # nan remains as nan\n",
    "\n",
    "        new_browser = 'other'\n",
    "\n",
    "        for known_browser in browser_list:\n",
    "            if known_browser in browser:\n",
    "                new_browser = known_browser\n",
    "                break\n",
    "\n",
    "        df.at[index, 'id_31'] = new_browser\n",
    "\n",
    "    if verbose:\n",
    "        print('browsers:', df.id_31.unique())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to generate aggregations\n",
    "\n",
    "- [reference](https://www.kaggle.com/artgor/eda-and-models#Feature-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aggregations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - df (pd.DataFrame): dataframe to generate aggregations on\n",
    "\n",
    "    return:\n",
    "    - dataframe with aggregations\n",
    "    \"\"\"\n",
    "    df['TransactionAmt_to_mean_card1'] = df['TransactionAmt'] / df.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "    df['TransactionAmt_to_mean_card4'] = df['TransactionAmt'] / df.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "    df['TransactionAmt_to_std_card1'] = df['TransactionAmt'] / df.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "    df['TransactionAmt_to_std_card4'] = df['TransactionAmt'] / df.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "    df['id_02_to_mean_card1'] = df['id_02'] / df.groupby(['card1'])['id_02'].transform('mean')\n",
    "    df['id_02_to_mean_card4'] = df['id_02'] / df.groupby(['card4'])['id_02'].transform('mean')\n",
    "    df['id_02_to_std_card1'] = df['id_02'] / df.groupby(['card1'])['id_02'].transform('std')\n",
    "    df['id_02_to_std_card4'] = df['id_02'] / df.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "    df['D15_to_mean_card1'] = df['D15'] / df.groupby(['card1'])['D15'].transform('mean')\n",
    "    df['D15_to_mean_card4'] = df['D15'] / df.groupby(['card4'])['D15'].transform('mean')\n",
    "    df['D15_to_std_card1'] = df['D15'] / df.groupby(['card1'])['D15'].transform('std')\n",
    "    df['D15_to_std_card4'] = df['D15'] / df.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "    df['D15_to_mean_addr1'] = df['D15'] / df.groupby(['addr1'])['D15'].transform('mean')\n",
    "    df['D15_to_mean_addr2'] = df['D15'] / df.groupby(['addr2'])['D15'].transform('mean')\n",
    "    df['D15_to_std_addr1'] = df['D15'] / df.groupby(['addr1'])['D15'].transform('std')\n",
    "    df['D15_to_std_addr2'] = df['D15'] / df.groupby(['addr2'])['D15'].transform('std')\n",
    "\n",
    "    df[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = df['P_emaildomain'].str.split('.', expand=True)\n",
    "    df[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = df['R_emaildomain'].str.split('.', expand=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function for preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, verbose: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - df (pd.DataFrame): dataframe that contains data for preprocessing\n",
    "    - verbose (bool): prints information if True\n",
    "\n",
    "    return:\n",
    "    - processed dataframe\n",
    "    \"\"\"\n",
    "    df = df.drop(useless_features, axis=1)\n",
    "\n",
    "    df = ignore_browser_version(df)\n",
    "    df = generate_aggregations(df)\n",
    "\n",
    "    num_categories_cutoff = 30\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            if df[column].nunique() <= num_categories_cutoff:\n",
    "                df[column]= df[column].astype('category')\n",
    "            else:\n",
    "                df[column] = df[column].astype(str)\n",
    "                df[column] = le.fit_transform(df[column])\n",
    "        else:\n",
    "            df[column] = df[column].fillna(df[column].median())\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define function to train LightGBM\n",
    "\n",
    "- using [reference notebook parameters](https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(df: pd.DataFrame) -> lgb.Booster:\n",
    "    \"\"\"\n",
    "    params:\n",
    "    - df (pd.DataFrame): data to be used to train lgbm\n",
    "\n",
    "    return:\n",
    "    - trained lightgbm classifier\n",
    "    \"\"\"\n",
    "    is_fraud_data = dataframe['isFraud']\n",
    "    features_dataframe = dataframe.drop('isFraud', axis=1)\n",
    "\n",
    "    train_features, val_features, train_target, val_target = train_test_split(\n",
    "        features_dataframe, \n",
    "        is_fraud_data, \n",
    "        test_size=0.1,\n",
    "    )\n",
    "\n",
    "    del features_dataframe\n",
    "    del is_fraud_data\n",
    "\n",
    "    train_data = lgb.Dataset(train_features, train_target)\n",
    "    val_data = lgb.Dataset(val_features, val_target)\n",
    "\n",
    "    del train_features\n",
    "    del train_target\n",
    "    del val_features\n",
    "    del val_target\n",
    "\n",
    "    params = {\n",
    "        'num_leaves': 491,\n",
    "        'min_child_weight': 0.03454472573214212,\n",
    "        'feature_fraction': 0.3797454081646243,\n",
    "        'bagging_fraction': 0.4181193142567742,\n",
    "        'min_data_in_leaf': 106,\n",
    "        'objective': 'binary',\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.006883242363721497,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'bagging_seed': 11,\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'reg_alpha': 0.3899927210061127,\n",
    "        'reg_lambda': 0.6485237330340494,\n",
    "        'random_state': 47,\n",
    "    }\n",
    "\n",
    "    return lgb.train(\n",
    "        params, \n",
    "        train_set=train_data, \n",
    "        num_boost_round=10000, \n",
    "        valid_sets=[train_data, val_data],\n",
    "        verbose_eval=2000,\n",
    "        early_stopping_rounds=500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mem. usage decreased to 542.35 Mb (69.4% reduction)\nMem. usage decreased to 25.86 Mb (42.7% reduction)\nnumber of rows in training data: 590540\nbrowsers: [nan 'samsung' 'safari' 'chrome' 'edge' 'firefox' 'ie' 'other' 'opera'\n 'aol' 'silk' 'waterfox' 'puffin' 'cyberfox' 'palemoon' 'maxthon' 'iron'\n 'seamonkey' 'comodo' 'chromium' 'icedragon']\nMem. usage decreased to 378.75 Mb (7.6% reduction)\nCPU times: user 1min 10s, sys: 36 s, total: 1min 46s\nWall time: 1min 46s\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   isFraud  TransactionDT  TransactionAmt ProductCD  card1  card2  card3  \\\n0        0          86400            68.5         W  13926  361.0  150.0   \n1        0          86401            29.0         W   2755  404.0  150.0   \n2        0          86469            59.0         W   4663  490.0  150.0   \n3        0          86499            50.0         W  18132  567.0  150.0   \n4        0          86506            50.0         H   4497  514.0  150.0   \n\n        card4  card5   card6  ...  D15_to_mean_addr1  D15_to_mean_addr2  \\\n0    discover  142.0  credit  ...           0.000000           0.000000   \n1  mastercard  102.0  credit  ...           0.000000           0.000000   \n2        visa  166.0   debit  ...           1.611328           1.721680   \n3  mastercard  117.0   debit  ...           0.686035           0.606445   \n4  mastercard  102.0  credit  ...           0.459473           0.458984   \n\n   D15_to_std_addr1  D15_to_std_addr2  P_emaildomain_1  P_emaildomain_2  \\\n0          0.000000          0.000000               25              NaN   \n1          0.000000          0.000000               15              com   \n2          1.486328          1.522461               28              com   \n3          0.576660          0.536621               43              com   \n4          0.408691          0.406006               15              com   \n\n   P_emaildomain_3  R_emaildomain_1  R_emaildomain_2  R_emaildomain_3  \n0              NaN               25              NaN              NaN  \n1              NaN               25              NaN              NaN  \n2              NaN               25              NaN              NaN  \n3              NaN               25              NaN              NaN  \n4              NaN               25              NaN              NaN  \n\n[5 rows x 292 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isFraud</th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>ProductCD</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card4</th>\n      <th>card5</th>\n      <th>card6</th>\n      <th>...</th>\n      <th>D15_to_mean_addr1</th>\n      <th>D15_to_mean_addr2</th>\n      <th>D15_to_std_addr1</th>\n      <th>D15_to_std_addr2</th>\n      <th>P_emaildomain_1</th>\n      <th>P_emaildomain_2</th>\n      <th>P_emaildomain_3</th>\n      <th>R_emaildomain_1</th>\n      <th>R_emaildomain_2</th>\n      <th>R_emaildomain_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>86400</td>\n      <td>68.5</td>\n      <td>W</td>\n      <td>13926</td>\n      <td>361.0</td>\n      <td>150.0</td>\n      <td>discover</td>\n      <td>142.0</td>\n      <td>credit</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>86401</td>\n      <td>29.0</td>\n      <td>W</td>\n      <td>2755</td>\n      <td>404.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>102.0</td>\n      <td>credit</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>15</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>86469</td>\n      <td>59.0</td>\n      <td>W</td>\n      <td>4663</td>\n      <td>490.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>166.0</td>\n      <td>debit</td>\n      <td>...</td>\n      <td>1.611328</td>\n      <td>1.721680</td>\n      <td>1.486328</td>\n      <td>1.522461</td>\n      <td>28</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>86499</td>\n      <td>50.0</td>\n      <td>W</td>\n      <td>18132</td>\n      <td>567.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>117.0</td>\n      <td>debit</td>\n      <td>...</td>\n      <td>0.686035</td>\n      <td>0.606445</td>\n      <td>0.576660</td>\n      <td>0.536621</td>\n      <td>43</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>86506</td>\n      <td>50.0</td>\n      <td>H</td>\n      <td>4497</td>\n      <td>514.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>102.0</td>\n      <td>credit</td>\n      <td>...</td>\n      <td>0.459473</td>\n      <td>0.458984</td>\n      <td>0.408691</td>\n      <td>0.406006</td>\n      <td>15</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 292 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataframe = load_training_data()\n",
    "dataframe = preprocess(dataframe)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training base classifier 0\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.9995\tvalid_1's auc: 0.974259\n[4000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.976622\n[6000]\ttraining's auc: 1\tvalid_1's auc: 0.976866\nEarly stopping, best iteration is:\n[5950]\ttraining's auc: 1\tvalid_1's auc: 0.976886\ntraining base classifier 1\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999527\tvalid_1's auc: 0.974274\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.97666\nEarly stopping, best iteration is:\n[5143]\ttraining's auc: 1\tvalid_1's auc: 0.976826\ntraining base classifier 2\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999505\tvalid_1's auc: 0.975836\n[4000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.977785\nEarly stopping, best iteration is:\n[3967]\ttraining's auc: 0.999997\tvalid_1's auc: 0.977801\ntraining base classifier 3\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999511\tvalid_1's auc: 0.974987\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.977389\n[6000]\ttraining's auc: 1\tvalid_1's auc: 0.977734\nEarly stopping, best iteration is:\n[5789]\ttraining's auc: 1\tvalid_1's auc: 0.977762\ntraining base classifier 4\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.99952\tvalid_1's auc: 0.971292\nEarly stopping, best iteration is:\n[3375]\ttraining's auc: 0.99999\tvalid_1's auc: 0.972633\ntraining base classifier 5\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999498\tvalid_1's auc: 0.973849\nEarly stopping, best iteration is:\n[3313]\ttraining's auc: 0.999985\tvalid_1's auc: 0.975407\ntraining base classifier 6\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999489\tvalid_1's auc: 0.971962\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.974107\nEarly stopping, best iteration is:\n[4603]\ttraining's auc: 1\tvalid_1's auc: 0.974329\ntraining base classifier 7\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999526\tvalid_1's auc: 0.972586\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.974929\nEarly stopping, best iteration is:\n[4368]\ttraining's auc: 0.999999\tvalid_1's auc: 0.974997\ntraining base classifier 8\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999514\tvalid_1's auc: 0.974213\n[4000]\ttraining's auc: 0.999999\tvalid_1's auc: 0.976544\nEarly stopping, best iteration is:\n[4203]\ttraining's auc: 0.999999\tvalid_1's auc: 0.976609\ntraining base classifier 9\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999494\tvalid_1's auc: 0.97394\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.976255\n[6000]\ttraining's auc: 1\tvalid_1's auc: 0.976564\nEarly stopping, best iteration is:\n[5889]\ttraining's auc: 1\tvalid_1's auc: 0.976583\ntraining base classifier 10\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999534\tvalid_1's auc: 0.972898\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.975021\nEarly stopping, best iteration is:\n[4518]\ttraining's auc: 1\tvalid_1's auc: 0.975119\ntraining base classifier 11\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.99952\tvalid_1's auc: 0.975338\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.977515\nEarly stopping, best iteration is:\n[5251]\ttraining's auc: 1\tvalid_1's auc: 0.977735\ntraining base classifier 12\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999478\tvalid_1's auc: 0.977176\n[4000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.979413\nEarly stopping, best iteration is:\n[4797]\ttraining's auc: 1\tvalid_1's auc: 0.979526\ntraining base classifier 13\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999497\tvalid_1's auc: 0.973983\n[4000]\ttraining's auc: 0.999997\tvalid_1's auc: 0.976785\nEarly stopping, best iteration is:\n[5299]\ttraining's auc: 1\tvalid_1's auc: 0.976908\ntraining base classifier 14\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999493\tvalid_1's auc: 0.977317\n[4000]\ttraining's auc: 0.999999\tvalid_1's auc: 0.979061\nEarly stopping, best iteration is:\n[3560]\ttraining's auc: 0.999994\tvalid_1's auc: 0.979104\ntraining base classifier 15\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999494\tvalid_1's auc: 0.973636\n[4000]\ttraining's auc: 0.999999\tvalid_1's auc: 0.975308\nEarly stopping, best iteration is:\n[4114]\ttraining's auc: 0.999999\tvalid_1's auc: 0.975321\ntraining base classifier 16\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999495\tvalid_1's auc: 0.976397\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.97869\nEarly stopping, best iteration is:\n[5333]\ttraining's auc: 1\tvalid_1's auc: 0.978793\ntraining base classifier 17\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999521\tvalid_1's auc: 0.975715\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.978256\n[6000]\ttraining's auc: 1\tvalid_1's auc: 0.97863\nEarly stopping, best iteration is:\n[6033]\ttraining's auc: 1\tvalid_1's auc: 0.978641\ntraining base classifier 18\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999511\tvalid_1's auc: 0.972776\nEarly stopping, best iteration is:\n[3415]\ttraining's auc: 0.99999\tvalid_1's auc: 0.974961\ntraining base classifier 19\nTraining until validation scores don't improve for 500 rounds\n[2000]\ttraining's auc: 0.999504\tvalid_1's auc: 0.969635\n[4000]\ttraining's auc: 0.999998\tvalid_1's auc: 0.971403\n[6000]\ttraining's auc: 1\tvalid_1's auc: 0.971631\nEarly stopping, best iteration is:\n[5930]\ttraining's auc: 1\tvalid_1's auc: 0.971657\nCPU times: user 3d 4h 7min 56s, sys: 52min 30s, total: 3d 5h 27s\nWall time: 7h 18min 51s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_base_classifiers = 20\n",
    "\n",
    "for index in range(num_base_classifiers):\n",
    "    print(f'training base classifier {index}')\n",
    "\n",
    "    classifier = train_lgbm(dataframe)\n",
    "    joblib.dump(classifier, model_dir_path + 'base_' + str(index) + '.joblib')  # save model\n",
    "\n",
    "    time.sleep(120)  # let my computer cool down, or it may overheat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mem. usage decreased to 472.59 Mb (68.9% reduction)\nMem. usage decreased to 25.44 Mb (42.7% reduction)\nnumber of rows in test data: 506691\nbrowsers: [nan 'chrome' 'ie' 'safari' 'edge' 'firefox' 'samsung' 'other' 'opera'\n 'palemoon']\nMem. usage decreased to 335.06 Mb (7.1% reduction)\nCPU times: user 59.7 s, sys: 30.8 s, total: 1min 30s\nWall time: 1min 30s\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   TransactionDT  TransactionAmt ProductCD  card1  card2  card3       card4  \\\n0       18403224       31.953125         W  10409  111.0  150.0        visa   \n1       18403263       49.000000         W   4272  111.0  150.0        visa   \n2       18403310      171.000000         W   4476  574.0  150.0        visa   \n3       18403310      285.000000         W  10989  360.0  150.0        visa   \n4       18403317       67.937500         W  18018  452.0  150.0  mastercard   \n\n   card5  card6  addr1  ...  D15_to_mean_addr1  D15_to_mean_addr2  \\\n0  226.0  debit  170.0  ...           1.557617           1.759766   \n1  226.0  debit  299.0  ...           3.033203           2.728516   \n2  226.0  debit  472.0  ...           0.457031           0.417480   \n3  166.0  debit  205.0  ...           1.020508           1.041016   \n4  117.0  debit  264.0  ...           0.098999           0.094666   \n\n   D15_to_std_addr1  D15_to_std_addr2  P_emaildomain_1  P_emaildomain_2  \\\n0               0.0               0.0               15              com   \n1               0.0               0.0                2              com   \n2               0.0               0.0               17              com   \n3               0.0               0.0               15              com   \n4               0.0               0.0               15              com   \n\n   P_emaildomain_3  R_emaildomain_1  R_emaildomain_2  R_emaildomain_3  \n0              NaN               25              NaN              NaN  \n1              NaN               25              NaN              NaN  \n2              NaN               25              NaN              NaN  \n3              NaN               25              NaN              NaN  \n4              NaN               25              NaN              NaN  \n\n[5 rows x 291 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionDT</th>\n      <th>TransactionAmt</th>\n      <th>ProductCD</th>\n      <th>card1</th>\n      <th>card2</th>\n      <th>card3</th>\n      <th>card4</th>\n      <th>card5</th>\n      <th>card6</th>\n      <th>addr1</th>\n      <th>...</th>\n      <th>D15_to_mean_addr1</th>\n      <th>D15_to_mean_addr2</th>\n      <th>D15_to_std_addr1</th>\n      <th>D15_to_std_addr2</th>\n      <th>P_emaildomain_1</th>\n      <th>P_emaildomain_2</th>\n      <th>P_emaildomain_3</th>\n      <th>R_emaildomain_1</th>\n      <th>R_emaildomain_2</th>\n      <th>R_emaildomain_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18403224</td>\n      <td>31.953125</td>\n      <td>W</td>\n      <td>10409</td>\n      <td>111.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>226.0</td>\n      <td>debit</td>\n      <td>170.0</td>\n      <td>...</td>\n      <td>1.557617</td>\n      <td>1.759766</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18403263</td>\n      <td>49.000000</td>\n      <td>W</td>\n      <td>4272</td>\n      <td>111.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>226.0</td>\n      <td>debit</td>\n      <td>299.0</td>\n      <td>...</td>\n      <td>3.033203</td>\n      <td>2.728516</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18403310</td>\n      <td>171.000000</td>\n      <td>W</td>\n      <td>4476</td>\n      <td>574.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>226.0</td>\n      <td>debit</td>\n      <td>472.0</td>\n      <td>...</td>\n      <td>0.457031</td>\n      <td>0.417480</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18403310</td>\n      <td>285.000000</td>\n      <td>W</td>\n      <td>10989</td>\n      <td>360.0</td>\n      <td>150.0</td>\n      <td>visa</td>\n      <td>166.0</td>\n      <td>debit</td>\n      <td>205.0</td>\n      <td>...</td>\n      <td>1.020508</td>\n      <td>1.041016</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18403317</td>\n      <td>67.937500</td>\n      <td>W</td>\n      <td>18018</td>\n      <td>452.0</td>\n      <td>150.0</td>\n      <td>mastercard</td>\n      <td>117.0</td>\n      <td>debit</td>\n      <td>264.0</td>\n      <td>...</td>\n      <td>0.098999</td>\n      <td>0.094666</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>15</td>\n      <td>com</td>\n      <td>NaN</td>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 291 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataframe, transaction_id_data = load_test_data()\n",
    "dataframe = preprocess(dataframe)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do inference and get output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "doing inference using base classifier 0\ndoing inference using base classifier 1\ndoing inference using base classifier 2\ndoing inference using base classifier 3\ndoing inference using base classifier 4\ndoing inference using base classifier 5\ndoing inference using base classifier 6\ndoing inference using base classifier 7\ndoing inference using base classifier 8\ndoing inference using base classifier 9\ndoing inference using base classifier 10\ndoing inference using base classifier 11\ndoing inference using base classifier 12\ndoing inference using base classifier 13\ndoing inference using base classifier 14\ndoing inference using base classifier 15\ndoing inference using base classifier 16\ndoing inference using base classifier 17\ndoing inference using base classifier 18\ndoing inference using base classifier 19\nCPU times: user 11h 9min 32s, sys: 1min 27s, total: 11h 11min\nWall time: 1h 5min 24s\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   TransactionID   isFraud\n0        3663549  0.000225\n1        3663550  0.000190\n2        3663551  0.000584\n3        3663552  0.000650\n4        3663553  0.000158",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TransactionID</th>\n      <th>isFraud</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3663549</td>\n      <td>0.000225</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3663550</td>\n      <td>0.000190</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3663551</td>\n      <td>0.000584</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3663552</td>\n      <td>0.000650</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3663553</td>\n      <td>0.000158</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f'doing inference using base classifier 0')\n",
    "classifier = joblib.load(model_dir_path + 'base_0.joblib')  # load model\n",
    "\n",
    "prediction = classifier.predict(dataframe)\n",
    "del classifier\n",
    "\n",
    "for index in range(1, num_base_classifiers):\n",
    "    print(f'doing inference using base classifier {index}')\n",
    "\n",
    "    classifier = joblib.load(model_dir_path + 'base_' + str(index) + '.joblib')  # load model\n",
    "    prediction += classifier.predict(dataframe)\n",
    "\n",
    "    if (index + 1) / 7 == 0:\n",
    "        time.sleep(60)  # let my computer cool down, or it may overheat\n",
    "\n",
    "del dataframe\n",
    "prediction /= num_base_classifiers\n",
    "\n",
    "output_dataframe = pd.DataFrame({\n",
    "    'TransactionID': transaction_id_data,\n",
    "    'isFraud': pd.Series(prediction),\n",
    "})\n",
    "output_dataframe.to_csv(output_path, index=False)\n",
    "\n",
    "output_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC\n",
    "\n",
    "- Kaggle public score: 0.943850\n",
    "  - leaderboard rank: 2744 out of 6381 (~43.00%)\n",
    "- Kaggle private score: 0.917212\n",
    "  - leaderboard rank: 2451 out of 6381 (~38.41%)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit0656ffb61a14454b8758eedef206058e",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}